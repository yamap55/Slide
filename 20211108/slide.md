<style type="text/css">
  .reveal h1,
  .reveal h2,
  .reveal h3,
  .reveal h4,
  .reveal h5,
  .reveal h6 {
    text-transform: none;
  }
  .reveal blockquote p {
    font-size: 32px;
  }
</style>

# PySpark ã§ãƒãƒã£ãŸä»¶ã«ã¤ã„ã¦

---

## ã‚¢ã‚¸ã‚§ãƒ³ãƒ€

- ã¯ã˜ã‚ã«
- ã‚„ã‚ŠãŸã„ã“ã¨
- ã‚³ãƒ¼ãƒ‰
- éšœå®³ç™ºç”Ÿ 1
- éšœå®³å¯¾å¿œ 2
- éšœå®³ç™ºç”Ÿ 2
- è§£èª¬
- éšœå®³å¯¾å¿œ 2
- æ€ã£ãŸã“ã¨

---

## ã¯ã˜ã‚ã«

--

- æ™®æ®µã‹ã‚‰ PySpark ä½¿ã£ã¦ã„ã‚‹äººã ã¨ã‚ã‚‹ã‚ã‚‹ã‹ã‚‚
- PySpark ã®å…ˆç”Ÿã«æœ¬ä»¶ã‚’ç›¸è«‡ã—ãŸã‚‰ä¸€ç¬ã§è¿”äº‹ãŒããŸ
- éšœå®³èª¿æŸ»ãŒã“ã® LT ã®ç¯„å›²

---

## ã‚„ã‚ŠãŸã„ã“ã¨

--

## å…ƒãƒ‡ãƒ¼ã‚¿

| id  | action |
| --- | ------ |
| 1   | A      |
| 1   | A      |
| 1   | B      |
| 2   | A      |
| 2   | C      |
| 2   | C      |
| 3   | B      |

--

## é›†è¨ˆ

| id  | action |
| --- | ------ |
| 1   | A      |
| 2   | C      |
| 3   | B      |

â€»ID æ¯ã«ä¸€ç•ªå¤šã„ Action ã‚’é¸æŠ

--

## åˆ¥ã® DF

| id  | type |
| --- | ---- |
| 1   | X    |
| 2   | Y    |
| 3   | Z    |

--

## æœ€çµ‚çµæœ

| id  | action | type |
| --- | ------ | ---- |
| 1   | A      | X    |
| 2   | C      | Y    |
| 3   | B      | Z    |

---

## ã‚³ãƒ¼ãƒ‰æ›¸ã„ã¦ã„ã

--

## å…ƒãƒ‡ãƒ¼ã‚¿ã®é›†è¨ˆ

| id  | action |
| --- | ------ |
| 1   | A      |
| 1   | A      |
| 1   | B      |
| 2   | A      |
| 2   | C      |
| 2   | C      |
| 3   | B      |

--

```python
df = (
  spark.read.parquet(path)
  .groupBy('id', 'action').count()
  .withColumn(
    'rank',
    F.row_number().over(
      Window().partitionBy('id').orderBy(F.col('count').desc())
    )
  )
  .where(F.col('rank') == 1)
)
```

â€»idã€action æ¯ã«æ•°ãˆã¦ 1 ç•ªå¤šã„å¥´ã ã‘ã‚’å–å¾—

--

## é›†è¨ˆã§ããŸ

| id  | action |
| --- | ------ |
| 1   | A      |
| 2   | C      |
| 3   | B      |

--

## åˆ¥ã® DF ã¨ join

```python
joined_df = df.join(other_df, 'id')
```

--

## join ã§ããŸ

| id  | action | type |
| --- | ------ | ---- |
| 1   | A      | X    |
| 2   | C      | Y    |
| 3   | B      | Z    |

--

# å®Œ

---

## éšœå®³ç™ºç”Ÿ ğŸ˜­

--

## ãƒ¡ãƒ¢ãƒªä¸è¶³

- å¯¾è±¡ã® DF ã¯å…±ã«ã‹ãªã‚Šå¤§ãã„
  - 5000 ä¸‡ä»¶ä»¥ä¸Š
- join æ™‚ã«ç¨€ã«ãƒ¡ãƒ¢ãƒªä¸è¶³ãŒèµ·å› ã¨æ€ã‚ã‚Œã‚‹ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ
  - æ­£ç¢ºã«ã¯ join å¾Œã« write ã™ã‚‹ã¨ç™ºç”Ÿ

â€»ã“ã“ã®èª¿æŸ»çµæœã«ã¤ã„ã¦ã¯æœ¬ LT ã®å¯¾è±¡å¤–

---

## éšœå®³å¯¾å¿œ

- join ã‚’ç´°ã‹ãè¡Œã£ã¦ union ã™ã‚‹
- å…ƒ DF ã‚’å°ã•ãã™ã‚‹äº‹ã§ä½¿ç”¨ãƒ¡ãƒ¢ãƒªãŒå‰Šæ¸›ã•ã‚Œã‚‹

â€»ã“ã“ã®å¯¾å¿œç­–ã«ã¤ã„ã¦ã¯æœ¬ LT ã®å¯¾è±¡å¤–

--

## ç´°ã‹ã join

```python
action_list = ['A', 'B', 'C', 'D', 'E']
l = [
  df.filter(F.col('action') == action).join(other_df, 'id')
  for action in action_list
]
joined_df = reduce(lambda x, y: x.union(y), l)
```

â€»ã“ã“ã®ã‚³ãƒ¼ãƒ‰ã¯æ­£ã—ã„

---

## éšœå®³ç™ºç”Ÿ ğŸ˜­

--

```python
count1 = joined_df.select('id').distinct().count()
count2 = joined_df.count()
print(count1, count2, count1 - count2)
```

```
954433 999928 -45495
```

â€»ID ãŒé‡è¤‡ã—ã¦ã„ã‚‹ï¼

--

## ä½•ãŒæ‚ªã‹ã£ãŸã‹ã‚ã‹ã‚‹äººï¼ï¼ğŸ™‹

---

## è§£èª¬

--

ã“ã“ã®ã‚³ãƒ¼ãƒ‰ãŒè‰¯ããªã„

```python
df = (
  spark.read.parquet(path)
  .groupBy('id', 'action').count()
  .withColumn(
    'rank',
    F.row_number().over(
      Window().partitionBy('id').orderBy(F.col('count').desc())
    )
  )
  .where(F.col('rank') == 1)
)
```

â€»idã€action æ¯ã«æ•°ãˆã¦ 1 ç•ªå¤šã„å¥´ã ã‘ã‚’å–å¾—

--

## å…ƒãƒ‡ãƒ¼ã‚¿

| id  | action |
| --- | ------ |
| 1   | A      |
| ... | ...    |
| 5   | A      |
| 5   | B      |
| 5   | C      |

â€»count ãŒåŒå€¤ã«ãªã£ãŸå ´åˆã€çµæœãŒä¸å®š  
â€»id5 ã¯ Aã€Bã€C ã©ã‚ŒãŒé¸æŠã•ã‚Œã‚‹ã‹ã‚ã‹ã‚‰ãªã„

--

## ç´°ã‹ã join ã™ã‚‹äº‹ã§è¡¨é¢åŒ–

```python
action_list = ['A', 'B', 'C', 'D', 'E']
l = [
  df.filter(F.col('action') == action).join(other_df, 'id')
  for action in action_list
]
joined_df = reduce(lambda x, y: x.union(y), l)
```

--

- PySpark ã¯ DF ã®ä¸­èº«ï¼Ÿã¯é…å»¶è©•ä¾¡ã•ã‚Œã‚‹
- é›†è¨ˆçµæœã‚’ cache ã—ã¦ã„ãªã„
- ç´°ã‹ã join ã™ã‚‹éš›ã€â†“ ã® df ãŒæ¯å›ç•°ãªã‚‹
  - `df.filter(F.col('action') == action)`
- id5 ã¯ A ã®æ™‚ã«ã‚‚ã€B ã®æ™‚ã«ã‚‚ç™»å ´ã™ã‚‹ã¨ã„ã†äº‹ãŒç™ºç”Ÿã™ã‚‹
- çµæœçš„ã« ID ã¯ãƒ¦ãƒ‹ãƒ¼ã‚¯ã«ãªã‚‰ãªã„

--

## ãŠã‚‚ã—ã‚ã„ã¨ã“ã‚

- æ™®é€šã« join ã—ãŸæ™‚ã«ã¯ã“ã®å•é¡Œã¯è¡¨é¢åŒ–ã—ãªã„
- df ãŒä¸å®šã§ã‚ã£ãŸã¨ã—ã¦ã‚‚ 1 å›ã§ join ã™ã‚Œã° ID é‡è¤‡ã«ã¯ç¹‹ãŒã‚‰ãªã„ãŸã‚

```python
joined_df = df.join(other_df, 'id')
```

---

## éšœå®³å¯¾å¿œ

--

## `cache()`

```python
df = (
  spark.read.parquet(path)
  .groupBy('id', 'action').count()
  .withColumn(
    'rank',
    F.row_number().over(
      Window().partitionBy('id').orderBy(F.col('count').desc())
    )
  )
  .where(F.col('rank') == 1)
).cache()
```

--

## çµæœã‚’å›ºå®š

- é›†è¨ˆã—ãŸã‚‰ãã‚Œã‚’å›ºå®šã—ã¦ã—ã¾ãˆã°ä½•åº¦ä½¿ã£ã¦ã‚‚åŒã˜å€¤ã«ãªã‚‹

--

## æ ¹æœ¬è§£æ±º

- æœ¬æ¥ã€è§£æ±ºã™ã¹ãäº‹é …ã¯é›†è¨ˆçµæœãŒä¸å®šã§ã‚ã‚‹äº‹
- ä»¥ä¸‹ã® orderBy ã§ count ä»¥å¤–ã‚’å«ã‚ã¦å›ºå®šã•ã‚Œã‚‹ã‚ˆã†ã«ã™ã¹ã

```python
Window().partitionBy('id').orderBy(F.col('count').desc())
```

â€»ãŒã€ä»Šå›ã¯å¯¾å¿œã‚’ã‚¹ã‚­ãƒƒãƒ—ã€‚ã€‚ã€‚

---

## æ€ã£ãŸã“ã¨

- ãƒ¬ãƒ“ãƒ¥ãƒ¼ã§æŒ‡æ‘˜ã§ãã‚‹æ°—ãŒã—ãªã„
- ä»–ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ã©ã†ã—ã¦ã„ã‚‹ã®ã‹ï¼Ÿ
